# ELT_pipeline
Data Preprocessing Pipeline (ETL) using Pandas & Scikit-Learn

ğŸ“Œ Overview

This project implements a data preprocessing pipeline using Pandas and Scikit-Learn to automate the ETL (Extract, Transform, Load) process.
It processes the Titanic dataset, applies missing value imputation, scaling, and encoding, and saves the cleaned data as a CSV file.

ğŸ“‚ Project Structure

ğŸ“ Data-Preprocessing-Pipeline  
â”‚â”€â”€ your_notebook.ipynb      # Jupyter Notebook with complete ETL pipeline  
â”‚â”€â”€ processed_data.csv       # Cleaned & processed dataset  
â”‚â”€â”€ README.md                # Project documentation


âš™ï¸ Installation

Make sure you have Jupyter Notebook installed in VS Code and required dependencies.

1ï¸âƒ£ Install Jupyter in VS Code (if not installed)

pip install notebook

2ï¸âƒ£ Install Required Libraries

pip install pandas scikit-learn-

ğŸ“Š Dataset Used

The dataset used in this project is the Titanic dataset, which is loaded directly from GitHub.

Source: Titanic Dataset - Data Science Dojo

Attributes:

Age, Fare â†’ Numerical features (scaled)

Sex, Embarked â†’ Categorical features (one-hot encoded)

ğŸ› ï¸ Features of the Pipeline

âœ… Extract: Reads data from GitHub
âœ… Transform:

Handles missing values using SimpleImputer

Applies StandardScaler to numerical data

Uses OneHotEncoder for categorical variables
âœ… Load: Saves processed data as processed_data.csv

ğŸš€ How to Run

1ï¸âƒ£ Open VS Code and navigate to your project folder.
2ï¸âƒ£ Open Jupyter Notebook (your_notebook.ipynb).
3ï¸âƒ£ Run all cells step by step.
4ï¸âƒ£ The processed CSV file (processed_data.csv) will be saved in the same folder.

ğŸ“Œ Output

After running the notebook, the processed dataset is saved as:
ğŸ“„ processed_data.csv â†’ Fully cleaned & transformed dataset.

ğŸ“§ Contact

For any queries, feel free to reach out.@navyashreensgr@gmail.com
